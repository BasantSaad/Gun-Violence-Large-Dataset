{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f838d9c",
   "metadata": {},
   "source": [
    "![My Screenshot](enough-is-enough-ending-gun-violence-together-vector.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59cd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, when, year, month, dayofweek, \\\n",
    "    lit, round as spark_round\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import countDistinct, col, desc, round, sum, count, mean, first, lit, when\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15d689",
   "metadata": {},
   "source": [
    "#### This section is responsible for initializing the Spark session, which serves as the entry point for using Spark functionality.\n",
    "# \n",
    "##### 1. The code first attempts to **stop any existing SparkSession** by calling `spark.stop()`. This is done inside a try-except block to prevent errors if no Spark session is currently active.\n",
    "##### 2. Next, a new SparkSession is **configured** and created:\n",
    "###### -   The application name is set to \"PublicSafetySafe\", which helps identify the Spark job.\n",
    "###### -    Certain **configuration options** are set specifically for experimentation and debugging:\n",
    "             \n",
    "###### 1.            \"spark.sql.adaptive.enabled\" and \"spark.sql.adaptive.coalescePartitions.enabled\" are set to \"false\" to disable adaptive query execution features. This can help achieve more predictable execution plans and resource allocation during testing.\n",
    "            \n",
    "###### 2.           \"spark.sql.autoBroadcastJoinThreshold\" is set to \"-1\" to completely disable automatic broadcast joins, preventing Spark from broadcasting small tables, which can be useful for debugging join behavior.\n",
    "             \n",
    "######  3.           \"spark.sql.shuffle.partitions\" is set to \"50\" to control the number of partitions produced when shuffling data, which can affect parallelism and performance.\n",
    "\n",
    "##### 3. After creating the SparkSession, the SparkContext's log level is set to **\"ERROR\" to suppress informational and warning logs**, so the output is less noisy and only shows errors, making the debugging process cleaner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629507e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PublicSafetySafe\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")  \n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a40d6",
   "metadata": {},
   "source": [
    "### **load The Dataset ğŸ”ƒ**\n",
    "##### show details about data strucure by `printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c9f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows 246939\n",
      "root\n",
      " |-- incident_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city_or_county: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- n_killed: string (nullable = true)\n",
      " |-- n_injured: string (nullable = true)\n",
      " |-- incident_url: string (nullable = true)\n",
      " |-- source_url: string (nullable = true)\n",
      " |-- incident_url_fields_missing: string (nullable = true)\n",
      " |-- congressional_district: string (nullable = true)\n",
      " |-- gun_stolen: string (nullable = true)\n",
      " |-- gun_type: string (nullable = true)\n",
      " |-- incident_characteristics: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- location_description: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- n_guns_involved: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- participant_age: string (nullable = true)\n",
      " |-- participant_age_group: string (nullable = true)\n",
      " |-- participant_gender: string (nullable = true)\n",
      " |-- participant_name: string (nullable = true)\n",
      " |-- participant_relationship: string (nullable = true)\n",
      " |-- participant_status: string (nullable = true)\n",
      " |-- participant_type: string (nullable = true)\n",
      " |-- sources: string (nullable = true)\n",
      " |-- state_house_district: string (nullable = true)\n",
      " |-- state_senate_district: string (nullable = true)\n",
      "\n",
      "+-----------+--------+--------------+--------------+--------------------+--------+---------+--------------------+--------------------+---------------------------+----------------------+--------------------+--------------------+------------------------+--------+--------------------+---------+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+------------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
      "|incident_id|    date|         state|city_or_county|             address|n_killed|n_injured|        incident_url|          source_url|incident_url_fields_missing|congressional_district|          gun_stolen|            gun_type|incident_characteristics|latitude|location_description|longitude|n_guns_involved|               notes|     participant_age|participant_age_group|  participant_gender|    participant_name|participant_relationship|  participant_status|    participant_type|             sources|state_house_district|state_senate_district|\n",
      "+-----------+--------+--------------+--------------+--------------------+--------+---------+--------------------+--------------------+---------------------------+----------------------+--------------------+--------------------+------------------------+--------+--------------------+---------+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+------------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
      "|     461105|1/1/2013|  Pennsylvania|    Mckeesport|1506 Versailles A...|       0|        4|http://www.gunvio...|http://www.post-g...|                      FALSE|                    14|                NULL|                NULL|    Shot - Wounded/In...| 40.3467|                NULL| -79.8559|           NULL|Julian Sims under...|               0::20| 0::Adult 18+||1::...|0::Male||1::Male|...|      0::Julian Sims|                    NULL|0::Arrested||1::I...|0::Victim||1::Vic...|http://pittsburgh...|                NULL|                 NULL|\n",
      "|     460726|1/1/2013|    California|     Hawthorne|13500 block of Ce...|       1|        3|http://www.gunvio...|http://www.dailyb...|                      FALSE|                    43|                NULL|                NULL|    Shot - Wounded/In...|  33.909|                NULL| -118.333|           NULL|Four Shot; One Ki...|               0::20| 0::Adult 18+||1::...|             0::Male|   0::Bernard Gillis|                    NULL|0::Killed||1::Inj...|0::Victim||1::Vic...|http://losangeles...|                  62|                   35|\n",
      "|     478855|1/1/2013|          Ohio|        Lorain|1776 East 28th St...|       1|        3|http://www.gunvio...|http://chronicle....|                      FALSE|                     9|0::Unknown||1::Un...|0::Unknown||1::Un...|    Shot - Wounded/In...| 41.4455|         Cotton Club| -82.1377|              2|                NULL|0::25||1::31||2::...| 0::Adult 18+||1::...|0::Male||1::Male|...|0::Damien Bell||1...|                    NULL|0::Injured, Unhar...|0::Subject-Suspec...|http://www.mornin...|                  56|                   13|\n",
      "|     478925|1/5/2013|      Colorado|        Aurora|16000 block of Ea...|       4|        0|http://www.gunvio...|http://www.dailyd...|                      FALSE|                     6|                NULL|                NULL|    Shot - Dead (murd...| 39.6518|                NULL| -104.802|           NULL|                NULL|0::29||1::33||2::...| 0::Adult 18+||1::...|0::Female||1::Mal...|0::Stacie Philbro...|                    NULL|0::Killed||1::Kil...|0::Victim||1::Vic...|http://denver.cbs...|                  40|                   28|\n",
      "|     478959|1/7/2013|North Carolina|    Greensboro|307 Mourning Dove...|       2|        2|http://www.gunvio...|http://www.journa...|                      FALSE|                     6|0::Unknown||1::Un...|0::Handgun||1::Ha...|    Shot - Wounded/In...|  36.114|                NULL| -79.9569|              2|Two firearms reco...|0::18||1::46||2::...| 0::Adult 18+||1::...|0::Female||1::Mal...|0::Danielle Imani...|               3::Family|0::Injured||1::In...|0::Victim||1::Vic...|http://myfox8.com...|                  62|                   27|\n",
      "+-----------+--------+--------------+--------------+--------------------+--------+---------+--------------------+--------------------+---------------------------+----------------------+--------------------+--------------------+------------------------+--------+--------------------+---------+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+------------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(r\"C:\\Users\\Gell G15\\.cache\\kagglehub\\datasets\\jameslko\\gun-violence-data\\versions\\1\\gun-violence-data_01-2013_03-2018.csv\")\n",
    "\n",
    "print(\"Number of Rows\", df.count())\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c2125",
   "metadata": {},
   "source": [
    "### **Filteration of Columns for Needed Data ğŸ§µ**  \n",
    "#### **Columns Are not used ğŸ—‘ï¸**\n",
    "| Column Name                     | Description |\n",
    "|---------------------------------|-------------|\n",
    "| **incident_id**                  | A unique ID number for each reported gun violence incident. (Not useful for our analysis ğŸ« ) |\n",
    "| **participant_name**             | Names of the participants (if available; often anonymized). |\n",
    "| **address**                      | The street address or general location where the event took place. (Not needed ,we selected what is useful) |\n",
    "| **state_house_district**         | The stateâ€™s house district number for that location. |\n",
    "| **state_senate_district**        | The stateâ€™s senate district number. |\n",
    "| **location_description**         | Additional context about where it occurred (e.g., Home, Street, Bar). |\n",
    "| **congressional_district**      | The U.S. congressional district where the incident occurred. |\n",
    "| **incident_url**                 | Link to the Gun Violence Archive (GVA) page with detailed info about this incident. |\n",
    "| **source_url**                   | URL of the news article(s) or reports used as a source. |\n",
    "| **incident_url_fields_missing**  | A flag indicating if any data fields were missing in the GVA entry. |\n",
    "| **sources**                      | List of media outlets or reports that confirmed the data. |\n",
    "| **notes**                        | Free-text field with extra information â€” can include motives, victim names, or event summary. |\n",
    "| **participant_age_group**        | Age group(s) such as Adult 18+, Teen 12-17, Child 0-11. (we can get it from another one ğŸš®)|\n",
    "| **participant_status**           | Outcome for each participant (Killed, Injured, Arrested, etc.). (we can get it from another one ğŸš®) |\n",
    "\n",
    "##### **Columns May be used in Future ğŸ”®**\n",
    "\n",
    "| Column Name                     | Description |\n",
    "|---------------------------------|-------------|\n",
    "| **gun_stolen**                   | Whether the gun used was reported stolen (Yes, No, or Unknown). |\n",
    "| **gun_type**                     | The type(s) of gun(s) involved (e.g., Handgun, Rifle, Shotgun). |\n",
    "| **n_guns_involved**              | Number of guns used in the incident. |\n",
    "| **incident_characteristics**     | Descriptive tags about what kind of event it was (e.g., Suicide Attempt, Home Invasion, Gang Involved, Accidental Shooting). |\n",
    "| **participant_age**              | Age(s) of the participants. |\n",
    "| **participant_type**             | Whether each person was a Victim, Suspect, or Subject-Suspect. |\n",
    "| **participant_gender**           | Gender(s) of the participants (Male, Female). |\n",
    "| **participant_relationship**     | Relationship between participants (e.g., Family, Stranger, Acquaintance). |\n",
    "\n",
    "\n",
    "##### **Columns Are usedğŸ‘Œ**\n",
    "| Column Name                     | Description |\n",
    "|---------------------------------|-------------|\n",
    "| **date**                         | The date when the incident occurred (e.g., 2015-06-17). |\n",
    "| **state**                        | The U.S. state where the incident happened (e.g., Texas, California). |\n",
    "| **city_or_county**               | The city or county of the incident location. |\n",
    "| **n_killed**                     | Number of people killed in the incident. |\n",
    "| **n_injured**                    | Number of people injured in the incident. |\n",
    "| **latitude**                     | Latitude coordinate of the incident. |\n",
    "| **longitude**                    | Longitude coordinate of the incident. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18c75ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 231754\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\n",
    "    \"date\", \"state\", \"city_or_county\", \"latitude\", \"longitude\", \"n_killed\", \"n_injured\"\n",
    ")\n",
    "df = df.na.drop()\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90121fca",
   "metadata": {},
   "source": [
    "### **Preprocessing and Data Cleaning ğŸ§¹**\n",
    "### When converting all data formats, you will encounter two problems â¬‡ï¸â¬‡ï¸ :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a043d",
   "metadata": {},
   "source": [
    "##### 1-**define Date column Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a777a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  '2/22/2013' (type: str)\n",
      "  '5/19/2013' (type: str)\n",
      "  '6/25/2013' (type: str)\n",
      "  '7/14/2013' (type: str)\n",
      "  '8/19/2013' (type: str)\n",
      "  '8/23/2013' (type: str)\n",
      "  '10/26/2013' (type: str)\n",
      "  '3/7/2014' (type: str)\n",
      "  '3/17/2014' (type: str)\n",
      "  '3/26/2014' (type: str)\n"
     ]
    }
   ],
   "source": [
    "#======================================================Check date format============================================================================\n",
    "# there is a problem with the date format , data in string format so we need to convert it to date format\n",
    "date_sample = df.select(\"date\").distinct().limit(10).collect()\n",
    "for row in date_sample:\n",
    "    print(f\"  '{row['date']}' (type: {type(row['date']).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7db31",
   "metadata": {},
   "source": [
    "##### 2-**there is one record that have url string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f620ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing non-numeric values: 231753\n"
     ]
    }
   ],
   "source": [
    "#======================================================Filteration the invalid values============================================================================\n",
    "# filter the invalid values in the latitude column , because it is url and not a number\n",
    "df_clean = df.filter(\n",
    "    col(\"latitude\").rlike(\"^-?[0-9]*\\\\.?[0-9]+$\")  \n",
    ")\n",
    "\n",
    "print(f\"Number of rows after removing non-numeric values: {df_clean.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd098e2",
   "metadata": {},
   "source": [
    "##### **Prepare all Data columns format** \n",
    "###### after the previous steps , we delete records that not able to convert as a stable format\n",
    "###### Now, easy convertion using **withColumn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b84c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the data format ğŸ’¯\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city_or_county: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- n_killed: integer (nullable = true)\n",
      " |-- n_injured: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#================================================================= data format ============================================================================\n",
    "#all the data is in the correct format \n",
    "df_clean = df_clean.withColumn(\n",
    "    \"date\", \n",
    "    to_date(col(\"date\"), \"M/d/yyyy\")  #  MM/dd/yyyy\n",
    ").withColumn(\"n_killed\", col(\"n_killed\").cast(IntegerType())) \\\n",
    " .withColumn(\"n_injured\", col(\"n_injured\").cast(IntegerType())) \\\n",
    " .withColumn(\"latitude\", col(\"latitude\").cast(DoubleType())) \\\n",
    " .withColumn(\"longitude\", col(\"longitude\").cast(DoubleType()))\n",
    " \n",
    "'''print(\"=== Check date format ===\")\n",
    "df_clean.select(\"date\").show(10)'''\n",
    "\n",
    "print(\"check the data format ğŸ’¯\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b5bf8",
   "metadata": {},
   "source": [
    "### **(Optional)** The next Step for failed data format conversion\n",
    "- by this step i covered these two columns problems\n",
    "    - there is a problem with the date format , data in string format so we need to convert it to date format\n",
    "    - filter the invalid values in the latitude column , because it is url and not a number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bfcc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================failed in data format conversion==============================\n",
    "# Check if there are any rows where the date conversion failed (date is null)\n",
    "# If there are, try an alternative method to convert the date column\n",
    "\n",
    "if df_clean.filter(col(\"date\").isNull()).count() > 0:\n",
    "    print(\"âš ï¸  There is an issue converting some dates, trying the alternative method...\")\n",
    "    \n",
    "    df_clean = df.filter(\n",
    "        col(\"latitude\").rlike(\"^-?[0-9]*\\\\.?[0-9]+$\")\n",
    "    )\n",
    "    \n",
    "    # Split the date into parts and convert it\n",
    "    from pyspark.sql.functions import split, expr\n",
    "    \n",
    "    df_clean = df_clean.withColumn(\"date_parts\", split(col(\"date\"), \"/\")) \\\n",
    "        .withColumn(\"month\", col(\"date_parts\").getItem(0).cast(IntegerType())) \\\n",
    "        .withColumn(\"day\", col(\"date_parts\").getItem(1).cast(IntegerType())) \\\n",
    "        .withColumn(\"year\", col(\"date_parts\").getItem(2).cast(IntegerType())) \\\n",
    "        .withColumn(\"date_formatted\", \n",
    "                   expr(\"make_date(year, month, day)\")) \\\n",
    "        .drop(\"date_parts\", \"month\", \"day\", \"year\")\n",
    "    \n",
    "# All conversions\n",
    "    df_clean = df_clean.withColumn(\"n_killed\", col(\"n_killed\").cast(IntegerType())) \\\n",
    "        .withColumn(\"n_injured\", col(\"n_injured\").cast(IntegerType())) \\\n",
    "        .withColumn(\"latitude\", col(\"latitude\").cast(DoubleType())) \\\n",
    "        .withColumn(\"longitude\", col(\"longitude\").cast(DoubleType())) \\\n",
    "        .withColumnRenamed(\"date_formatted\", \"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82fd81",
   "metadata": {},
   "source": [
    "#### **Shown** the data after cleaning and Data format Conversion ğŸ’¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1243640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Data Verification ===\n",
      "number of row after cleaning :231753\n",
      "\n",
      "\n",
      "sample of data after cleaning\n",
      "+----------+--------+---------+--------+---------+\n",
      "|      date|latitude|longitude|n_killed|n_injured|\n",
      "+----------+--------+---------+--------+---------+\n",
      "|2013-01-01| 40.3467| -79.8559|       0|        4|\n",
      "|2013-01-01|  33.909| -118.333|       1|        3|\n",
      "|2013-01-01| 41.4455| -82.1377|       1|        3|\n",
      "|2013-01-05| 39.6518| -104.802|       4|        0|\n",
      "|2013-01-07|  36.114| -79.9569|       2|        2|\n",
      "|2013-01-07| 36.2405| -95.9768|       4|        0|\n",
      "|2013-01-19| 34.9791| -106.716|       5|        0|\n",
      "|2013-01-21| 29.9435| -90.0836|       0|        5|\n",
      "|2013-01-21| 37.9656| -121.718|       0|        4|\n",
      "|2013-01-23| 39.2899| -76.6412|       1|        6|\n",
      "+----------+--------+---------+--------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Final Data Verification\n",
    "print(\"=== Final Data Verification ===\")\n",
    "print(f\"number of row after cleaning :{df_clean.count()}\")\n",
    "\n",
    "print(\"\\n\\nsample of data after cleaning\")\n",
    "df_clean.select(\"date\", \"latitude\", \"longitude\", \"n_killed\", \"n_injured\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33075990",
   "metadata": {},
   "source": [
    "### **'''Define target variable (hotspots)'''ğŸ¥…**\n",
    "#### Now,\n",
    "- This code is creating a **new dataset** called df_with_target based on the cleaned data (df_clean).\n",
    "    - It **creates a new column** called \"total_victims\", which **sums** up the number of people killed (\"n_killed\") and the number of people injured (\"n_injured\") for each incident.\n",
    "\n",
    "    \n",
    "    -  It then **creates another column** called **\"IsHotspot\"**:(Target (when 5->1))\n",
    "        - If the **total number of victims** (killed + injured) is **greater than or equal to 5**, OR if the number of people **killed alone is at least 2**, **\"IsHotspot\" will be set to 1**.\n",
    "        - Otherwise, \"IsHotspot\" will be set to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b11c02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Target Variable (Hotspots) \n",
    "df_with_target = df_clean.withColumn(\n",
    "    \"total_victims\", col(\"n_killed\") + col(\"n_injured\")\n",
    ").withColumn(\n",
    "    \"IsHotspot\", \n",
    "    when((col(\"total_victims\") >= 5) | (col(\"n_killed\") >= 2), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca2975",
   "metadata": {},
   "source": [
    "#### This code calculates and displays summary statistics for the \"hotspots\" in the dataset.\n",
    " - For each group **(hotspot->1 and non-hotspot->0)**, it computes:\n",
    "   - The **total** number of **incidents** (count)\n",
    "   - The **average** number of people **killed per incident** (avg_killed)\n",
    "   - The **average** number of people **injured per incident** (avg_injured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6025d86",
   "metadata": {},
   "source": [
    "# **pumbğŸ’¥**\n",
    "### can you define the problem ?? ğŸ«  \n",
    "##### *think ,man*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40fd1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø³Ø§Ø®Ù†Ø© ===\n",
      "+---------+------+-------------------+------------------+\n",
      "|IsHotspot| count|         avg_killed|       avg_injured|\n",
      "+---------+------+-------------------+------------------+\n",
      "|        0|226090|0.20117209960635146|0.4863328762882038|\n",
      "|        1|  5663|  2.065159809288363|0.8087586085113897|\n",
      "+---------+------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hotspot Statistics\n",
    "\n",
    "hotspot_stats = df_with_target.groupBy(\"IsHotspot\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"n_killed\").alias(\"avg_killed\"),\n",
    "    avg(\"n_injured\").alias(\"avg_injured\")\n",
    ")\n",
    "print(\"=== Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø³Ø§Ø®Ù†Ø© ===\")\n",
    "hotspot_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f62450",
   "metadata": {},
   "source": [
    "### **Temporal Feature EngineeringğŸ“…**\n",
    "##### What is done:\n",
    "   - This cell extracts new temporal features from the \"date\" column, adding columns for year, month, day of the week, and day of the month to the DataFrame.\n",
    "##### Why is this done:\n",
    "   - Temporal features help machine learning models learn and utilize seasonal or periodic patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e40ceb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display a preview to verify the extracted temporal features\n",
      "+----------+----+-----+-----------+------------+\n",
      "|      date|year|month|day_of_week|day_of_month|\n",
      "+----------+----+-----+-----------+------------+\n",
      "|2013-01-01|2013|    1|          3|           1|\n",
      "|2013-01-01|2013|    1|          3|           1|\n",
      "|2013-01-01|2013|    1|          3|           1|\n",
      "|2013-01-05|2013|    1|          7|           5|\n",
      "|2013-01-07|2013|    1|          2|           7|\n",
      "|2013-01-07|2013|    1|          2|           7|\n",
      "|2013-01-19|2013|    1|          7|          19|\n",
      "|2013-01-21|2013|    1|          2|          21|\n",
      "|2013-01-21|2013|    1|          2|          21|\n",
      "|2013-01-23|2013|    1|          4|          23|\n",
      "+----------+----+-----+-----------+------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Temporal Feature Engineering\n",
    "\n",
    "df_features = df_with_target.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"date\"))) \\\n",
    "    .withColumn(\"day_of_month\", dayofmonth(col(\"date\")))\n",
    "\n",
    "print('Display a preview to verify the extracted temporal features')\n",
    "df_features.select(\"date\", \"year\", \"month\", \"day_of_week\", \"day_of_month\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af2857",
   "metadata": {},
   "source": [
    "### **LAt and LOn Feature Engineering ğŸ—ºï¸**\n",
    "- **What** it does: Adds **two new columns** (lat_grid, lon_grid) to df_features by **rounding** latitude and longitude to 3 decimal places.\n",
    "- **Why**: Rounding creates a spatial grid (binning) so **nearby points fall into the same cell**. This **reduces** cardinality and makes aggregations like counts, heatmaps, and clustering **fast and scalable in Spark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3c9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Spatial Feature Engineering (Grid Clustering)\n",
    "df_spatial = df_features.withColumn(\n",
    "    \"lat_grid\", round(col(\"latitude\"), 3)\n",
    ").withColumn(\n",
    "    \"lon_grid\", round(col(\"longitude\"), 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273a91b",
   "metadata": {},
   "source": [
    "#### Calculate Grid (Spatial) **Statistics**ğŸ“\n",
    "- Here we are **grouping** the data by spatial grids defined by **latitude and longitude** (lat_grid, lon_grid).\n",
    "    - For each grid cell, we calculate: **such as hotspot**\n",
    "       - incident_count: total number of incidents in that grid cell\n",
    "       - total_killed: total number of people killed in that grid cell\n",
    "       - total_injured: total number of people injured in that grid cell\n",
    "       - avg_killed_per_incident: average number of killed per incident in that cell\n",
    "#### Why? \n",
    "- Calculating these statistics per grid cell **helps us identify spatial patterns**, such as accident hotspots or areas with severe outcomes.\n",
    "- This summary can be used for heatmaps or as **features for further modeling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a4ca422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Grid Aggregation Statistics ===\n",
      "+--------+--------+--------------+------------+-------------+-----------------------+\n",
      "|lat_grid|lon_grid|incident_count|total_killed|total_injured|avg_killed_per_incident|\n",
      "+--------+--------+--------------+------------+-------------+-----------------------+\n",
      "|  33.636| -84.433|           252|           0|            0|                    0.0|\n",
      "|  39.294|  -76.62|           235|           3|           83|    0.01276595744680851|\n",
      "|  29.987| -95.348|           169|           0|            0|                    0.0|\n",
      "|  32.898|  -97.04|           160|           0|            1|                    0.0|\n",
      "|  33.435|-112.006|           159|           0|            0|                    0.0|\n",
      "|  38.908| -77.018|           134|           6|           11|    0.04477611940298507|\n",
      "|  29.955| -90.075|           109|           8|           41|    0.07339449541284404|\n",
      "|  28.436| -81.307|           104|           0|            1|                    0.0|\n",
      "|   39.85|-104.674|           102|           0|            0|                    0.0|\n",
      "|  47.661|-117.432|            99|           1|            2|   0.010101010101010102|\n",
      "+--------+--------+--------------+------------+-------------+-----------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Calculate Grid (Spatial) Statistics\n",
    "\n",
    "grid_counts = df_spatial.groupBy(\"lat_grid\", \"lon_grid\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"incident_count\"),\n",
    "        sum(\"n_killed\").alias(\"total_killed\"),\n",
    "        sum(\"n_injured\").alias(\"total_injured\"),\n",
    "        avg(\"n_killed\").alias(\"avg_killed_per_incident\")\n",
    "    )\n",
    "\n",
    "print(\"=== Grid Aggregation Statistics ===\")\n",
    "# Show the top 10 grid cells with the most incidents\n",
    "grid_counts.orderBy(desc(\"incident_count\")).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cccfebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Grid Statistics\n",
    "# What: Merge the calculated grid statistics (incident_count, total_killed, total_injured, avg_killed_per_incident) into the main dataframe, \n",
    "#       so every incident record gets these aggregate features from its spatial \"grid cell\".\n",
    "# Why: This enriches every row with additional spatial context. It's valuable for analysis and modeling, letting us use \"grid features\" as predictors.\n",
    "df_final = df_spatial.join(grid_counts, [\"lat_grid\", \"lon_grid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330def8",
   "metadata": {},
   "source": [
    "# **Ø§Ù„Ù…Ù†Ø³ÙŠ**\n",
    "### We gave our attention to numerical data and later to categorical data. Don't be sad, now we are giving it all our attention. ğŸ‘\n",
    "### **Handel unkown data if State and city_or_county columns (Categorical Data)** ğŸš¦ğŸ«·\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a21917a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Categorical Data\n",
    "\n",
    "df_final = df_final.fillna({\n",
    "    \"state\": \"Unknown\",\n",
    "    \"city_or_county\": \"Unknown\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7fc7fd",
   "metadata": {},
   "source": [
    "# **FINALLy**\n",
    "###### **Not the end , because X have no endğŸ¦¦**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03a7bbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ  ===\n",
      "+----------+--------+---------+--------+---------+---------+--------+--------+--------------+-------+\n",
      "|      date|latitude|longitude|n_killed|n_injured|IsHotspot|lat_grid|lon_grid|incident_count|  state|\n",
      "+----------+--------+---------+--------+---------+---------+--------+--------+--------------+-------+\n",
      "|2015-09-04| 19.4475| -155.189|       0|        0|        0|  19.448|-155.189|             1| Hawaii|\n",
      "|2016-12-20| 25.5399| -80.5126|       0|        1|        0|   25.54| -80.513|             1|Florida|\n",
      "|2014-01-03| 25.5514| -80.4047|       0|        0|        0|  25.551| -80.405|             1|Florida|\n",
      "|2014-09-02| 25.6077| -80.3734|       0|        1|        0|  25.608| -80.373|             1|Florida|\n",
      "|2014-08-31| 25.6864| -80.3804|       0|        1|        0|  25.686|  -80.38|             1|Florida|\n",
      "|2016-01-26|  25.727| -80.2496|       0|        2|        0|  25.727|  -80.25|             1|Florida|\n",
      "|2017-01-02| 25.7281|  -80.248|       0|        3|        0|  25.728| -80.248|             1|Florida|\n",
      "|2016-09-12| 25.7604| -80.2746|       0|        0|        0|   25.76| -80.275|             1|Florida|\n",
      "|2016-12-03| 25.7622| -80.3819|       0|        3|        0|  25.762| -80.382|             1|Florida|\n",
      "|2014-11-11| 25.7658| -80.2879|       0|        2|        0|  25.766| -80.288|             1|Florida|\n",
      "|2016-06-19| 25.7842| -80.1356|       0|        0|        0|  25.784| -80.136|             1|Florida|\n",
      "|2016-06-24| 25.7855| -80.2174|       0|        0|        0|  25.786| -80.217|             1|Florida|\n",
      "|2018-01-03| 25.7903| -80.2515|       0|        0|        0|   25.79| -80.252|             1|Florida|\n",
      "|2017-12-04| 25.7987| -80.2172|       1|        0|        0|  25.799| -80.217|             1|Florida|\n",
      "|2014-11-13| 25.8062| -80.2141|       1|        0|        0|  25.806| -80.214|             1|Florida|\n",
      "+----------+--------+---------+--------+---------+---------+--------+--------+--------------+-------+\n",
      "only showing top 15 rows\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Display Final Results\n",
    "\n",
    "print(\"=== Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ  ===\")\n",
    "df_final.select(\n",
    "    \"date\", \"latitude\", \"longitude\", \"n_killed\", \"n_injured\", \n",
    "    \"IsHotspot\", \"lat_grid\", \"lon_grid\", \"incident_count\", \"state\"\n",
    ").show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd714b",
   "metadata": {},
   "source": [
    "# **The same pumb** ğŸ’¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ac9fbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Data Summary ===\n",
      "Total rows: 231753\n",
      "Number of hotspots: 5663\n",
      "Percentage of hotspots: 2.44%\n"
     ]
    }
   ],
   "source": [
    "# Final Summary Statistics\n",
    "print(\"=== Final Data Summary ===\")\n",
    "print(f\"Total rows: {df_final.count()}\")\n",
    "print(f\"Number of hotspots: {df_final.filter(col('IsHotspot') == 1).count()}\")\n",
    "print(f\"Percentage of hotspots: {(df_final.filter(col('IsHotspot') == 1).count() / df_final.count() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb38e0",
   "metadata": {},
   "source": [
    "### **Oversampling** with SMOTE-like approach\n",
    "#### are you getting it?ğŸ‘ pravo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09d1f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Imbalanced Data =====\n",
      "\n",
      " Original class distribution:â­•\n",
      "+---------+------+\n",
      "|IsHotspot| count|\n",
      "+---------+------+\n",
      "|        0|226090|\n",
      "|        1|  5663|\n",
      "+---------+------+\n",
      "\n",
      "Class 0 (Non-Hotspot): 226,090 (97.56%)\n",
      "Class 1 (Hotspot): 5,663 (2.44%)\n",
      "Ratio: 39.9:1\n",
      "\n",
      "ğŸ”„ Applying oversampling to the minority class...\n",
      "Oversampling factor: 13.31x\n",
      "\n",
      " Distribution after oversampling:â˜‘ï¸\n",
      "+---------+------+\n",
      "|IsHotspot| count|\n",
      "+---------+------+\n",
      "|        0|226090|\n",
      "|        1| 75587|\n",
      "+---------+------+\n",
      "\n",
      "Class 0 (Non-Hotspot): 226,090 (74.94%)\n",
      "Class 1 (Hotspot): 75,587 (25.06%)\n",
      "New Ratio: 3.0:1\n",
      "\n",
      "âœ… Balanced dataset created successfully!\n",
      "Total rows: 301,677\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Balance Classes Using Oversampling \n",
    "print(\"===== Imbalanced Data =====\")\n",
    "#==================================================== Show original class distribution ==========================================================\n",
    "print(\"\\n Original class distribution:â­•\")\n",
    "original_distribution = df_with_target.groupBy(\"IsHotspot\").count().orderBy(\"IsHotspot\")\n",
    "original_distribution.show()\n",
    "\n",
    "hotspot_count = df_with_target.filter(col(\"IsHotspot\") == 1).count()\n",
    "non_hotspot_count = df_with_target.filter(col(\"IsHotspot\") == 0).count()\n",
    "\n",
    "print(f\"Class 0 (Non-Hotspot): {non_hotspot_count:,} ({non_hotspot_count/(non_hotspot_count+hotspot_count)*100:.2f}%)\")\n",
    "print(f\"Class 1 (Hotspot): {hotspot_count:,} ({hotspot_count/(non_hotspot_count+hotspot_count)*100:.2f}%)\")\n",
    "print(f\"Ratio: {non_hotspot_count/hotspot_count:.1f}:1\")\n",
    "#======================================================== Ø§Ù„Ù„ÙŠ Ø¹Ø§ÙˆØ² Ø¯Ø§ ÙŠØ±ÙˆØ­ ÙƒØ¯Ø§ ÙˆØ§Ù„Ù„ÙŠ Ø¹Ø§ÙˆØ² Ø¯Ø§ ÙŠØ¬ÙŠ ÙƒØ¯Ø§====================================\n",
    "\n",
    "# Split by class\n",
    "hotspot_df = df_with_target.filter(col(\"IsHotspot\") == 1)\n",
    "non_hotspot_df = df_with_target.filter(col(\"IsHotspot\") == 0)\n",
    "\n",
    "print(\"\\nğŸ”„ Applying oversampling to the minority class...\")\n",
    "\n",
    "#===================================================  Calculate oversampling factor for target ratio (e.g., 1:3 or 1:2, better than 1:40)\n",
    "\n",
    "target_ratio = 3  # Each hotspot will be repeated to achieve a 1:3 ratio\n",
    "oversample_factor = (non_hotspot_count / hotspot_count) / target_ratio\n",
    "\n",
    "print(f\"Oversampling factor: {oversample_factor:.2f}x\")\n",
    "\n",
    "#=================================================  Perform oversampling with replacement\n",
    "hotspot_oversampled = hotspot_df.sample(withReplacement=True, fraction=oversample_factor, seed=42)\n",
    "\n",
    "# Combine to form balanced DataFrame\n",
    "df_balanced = non_hotspot_df.union(hotspot_oversampled)\n",
    "\n",
    "#========================================================== Show new distribution\n",
    "print(\"\\n Distribution after oversampling:â˜‘ï¸\")\n",
    "balanced_distribution = df_balanced.groupBy(\"IsHotspot\").count().orderBy(\"IsHotspot\")\n",
    "balanced_distribution.show()\n",
    "\n",
    "hotspot_count_new = df_balanced.filter(col(\"IsHotspot\") == 1).count()\n",
    "non_hotspot_count_new = df_balanced.filter(col(\"IsHotspot\") == 0).count()\n",
    "\n",
    "print(f\"Class 0 (Non-Hotspot): {non_hotspot_count_new:,} ({non_hotspot_count_new/(non_hotspot_count_new+hotspot_count_new)*100:.2f}%)\")\n",
    "print(f\"Class 1 (Hotspot): {hotspot_count_new:,} ({hotspot_count_new/(non_hotspot_count_new+hotspot_count_new)*100:.2f}%)\")\n",
    "print(f\"New Ratio: {non_hotspot_count_new/hotspot_count_new:.1f}:1\")\n",
    "\n",
    "print(\"\\nâœ… Balanced dataset created successfully!\")\n",
    "print(f\"Total rows: {df_balanced.count():,}\")\n",
    "\n",
    "# Replace df_final with the balanced data\n",
    "df_final = df_balanced\n",
    "\n",
    "# print(\"\\nâš ï¸ Note: Use the balanced df_final in the next modeling steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4f4ad",
   "metadata": {},
   "source": [
    "### **Ø§ØªÙ†ÙƒØ±** Ù…Ø¹Ù„Ø´\n",
    "#### Solutions to Reduce Overfitting Risk:\n",
    "- Add Noise/Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9741df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, randn\n",
    "\n",
    "# Add small random noise to numerical features\n",
    "hotspot_oversampled = hotspot_df.sample(withReplacement=True, fraction=oversample_factor, seed=42) \\\n",
    "    .withColumn(\"latitude\", col(\"latitude\") + (randn(seed=42) * 0.001)) \\\n",
    "    .withColumn(\"longitude\", col(\"longitude\") + (randn(seed=43) * 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b08593",
   "metadata": {},
   "source": [
    "### **(OPtional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c32c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ÙØ­Øµ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ===\n",
      "Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: ['date', 'state', 'city_or_county', 'latitude', 'longitude', 'n_killed', 'n_injured', 'total_victims', 'IsHotspot']\n",
      "\n",
      "=== Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© ===\n",
      "âš ï¸ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§...\n",
      "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©\n",
      "\n",
      "=== Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© ===\n",
      "âš ï¸ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§...\n",
      "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©\n",
      "\n",
      "=== Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ© ===\n",
      "âš ï¸ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§...\n",
      "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ©\n",
      "\n",
      "âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¬Ø§Ù‡Ø²Ø©! Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: ['lat_grid', 'lon_grid', 'date', 'state', 'city_or_county', 'latitude', 'longitude', 'n_killed', 'n_injured', 'total_victims', 'IsHotspot', 'year', 'month', 'day_of_week', 'day_of_month', 'incident_count', 'total_killed', 'total_injured', 'avg_killed_per_incident']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Ø§Ù„Ø¬Ø²Ø¡ 1: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "# ============================================\n",
    "\n",
    "# Cell 1: Check Current Columns\n",
    "print(\"=== ÙØ­Øµ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ===\")\n",
    "print(f\"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {df_final.columns}\")\n",
    "\n",
    "# Cell 2: Create Temporal Features if Missing\n",
    "print(\"\\n=== Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© ===\")\n",
    "\n",
    "# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "if \"month\" not in df_final.columns:\n",
    "    print(\"âš ï¸ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§...\")\n",
    "    \n",
    "    from pyspark.sql.functions import year, month, dayofweek, dayofmonth\n",
    "    \n",
    "    df_final = df_final.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "        .withColumn(\"day_of_week\", dayofweek(col(\"date\"))) \\\n",
    "        .withColumn(\"day_of_month\", dayofmonth(col(\"date\")))\n",
    "    \n",
    "    print(\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©\")\n",
    "else:\n",
    "    print(\"âœ… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„\")\n",
    "\n",
    "# Cell 3: Create Spatial Features if Missing\n",
    "print(\"\\n=== Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© ===\")\n",
    "\n",
    "if \"lat_grid\" not in df_final.columns:\n",
    "    print(\"âš ï¸ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§...\")\n",
    "    \n",
    "    from pyspark.sql.functions import round as spark_round\n",
    "    \n",
    "    df_final = df_final.withColumn(\"lat_grid\", spark_round(col(\"latitude\"), 3)) \\\n",
    "        .withColumn(\"lon_grid\", spark_round(col(\"longitude\"), 3))\n",
    "    \n",
    "    print(\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©\")\n",
    "else:\n",
    "    print(\"âœ… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„\")\n",
    "\n",
    "# Cell 4: Create Grid Statistics if Missing\n",
    "print(\"\\n=== Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ© ===\")\n",
    "\n",
    "if \"incident_count\" not in df_final.columns:\n",
    "    print(\"âš ï¸ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§...\")\n",
    "    \n",
    "    from pyspark.sql.functions import count, sum as spark_sum, avg\n",
    "    \n",
    "    # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø­ÙˆØ§Ø¯Ø« ÙÙŠ ÙƒÙ„ Ø´Ø¨ÙƒØ©\n",
    "    grid_counts = df_final.groupBy(\"lat_grid\", \"lon_grid\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"incident_count\"),\n",
    "            spark_sum(\"n_killed\").alias(\"total_killed\"),\n",
    "            spark_sum(\"n_injured\").alias(\"total_injured\"),\n",
    "            avg(\"n_killed\").alias(\"avg_killed_per_incident\")\n",
    "        )\n",
    "    \n",
    "    # Ø¯Ù…Ø¬ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©\n",
    "    df_final = df_final.join(grid_counts, [\"lat_grid\", \"lon_grid\"])\n",
    "    \n",
    "    print(\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ©\")\n",
    "else:\n",
    "    print(\"âœ… Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„\")\n",
    "\n",
    "print(f\"\\nâœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¬Ø§Ù‡Ø²Ø©! Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {df_final.columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331805a9",
   "metadata": {},
   "source": [
    "### **Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99fea6",
   "metadata": {},
   "source": [
    "#### presteps â˜‘ï¸\n",
    "- Enssure all of Featrues are available\n",
    "- If not , create the columns to handle the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fdd0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['month', 'day_of_week', 'day_of_month', 'lat_grid', 'lon_grid', 'incident_count']\n"
     ]
    }
   ],
   "source": [
    "# step 1: Feature Selection and Assembly Setup\n",
    "feature_columns = [\n",
    "    \"month\", \"day_of_week\", \"day_of_month\",\n",
    "    \"lat_grid\", \"lon_grid\", \"incident_count\"\n",
    "]\n",
    "\n",
    "# Add categorical columns if available\n",
    "# Just ensure\n",
    "try:\n",
    "    if \"state_encoded\" in df_final.columns:\n",
    "        feature_columns.append(\"state_encoded\")\n",
    "    if \"city_or_county_encoded\" in df_final.columns:\n",
    "        feature_columns.append(\"city_or_county_encoded\")\n",
    "except:\n",
    "    print(\"âš ï¸  Categorical columns not found, proceeding without them\")\n",
    "\n",
    "print(f\"Features: {feature_columns}\")\n",
    "\n",
    "\n",
    "#   ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "missing_features = [f for f in feature_columns if f not in df_final.columns]\n",
    "if missing_features:\n",
    "    print(f\"âš ï¸ ØªØ­Ø°ÙŠØ±: Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_features}\")\n",
    "    feature_columns = [f for f in feature_columns if f in df_final.columns]\n",
    "    print(f\"Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6095467",
   "metadata": {},
   "source": [
    "### **Train & Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94770e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ÙŠÙ†Ø© 50% Ù„Ù„ØªØ¯Ø±ÙŠØ¨\n",
      "Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: 105722\n",
      "Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: 45028\n",
      " ØªÙˆØ²ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n",
      "+---------+-----+\n",
      "|IsHotspot|count|\n",
      "+---------+-----+\n",
      "|        0|79017|\n",
      "|        1|26705|\n",
      "+---------+-----+\n",
      "\n",
      " ØªÙˆØ²ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\n",
      "+---------+-----+\n",
      "|IsHotspot|count|\n",
      "+---------+-----+\n",
      "|        0|33860|\n",
      "|        1|11168|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Cell 8: Data Sampling and Split\n",
    "data_count = df_final.count()\n",
    "\n",
    "if data_count > 100000:\n",
    "    print(\"âš ï¸  Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ÙŠÙ†Ø© 50% Ù„Ù„ØªØ¯Ø±ÙŠØ¨\")\n",
    "    sample_df = df_final.sample(0.5, seed=42)\n",
    "else:\n",
    "    sample_df = df_final\n",
    "\n",
    "#split data set to train and test\n",
    "train_data, test_data = sample_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train_data.count()}\")\n",
    "print(f\"Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_data.count()}\")\n",
    "\n",
    "# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
    "print(\" ØªÙˆØ²ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "train_data.groupBy(\"IsHotspot\").count().show()\n",
    "print(\" ØªÙˆØ²ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "test_data.groupBy(\"IsHotspot\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999610a3",
   "metadata": {},
   "source": [
    "### **Feature Assembly**\n",
    "\n",
    "- Feature assembly is the process of **combining multiple feature columns into a single feature vector column**\n",
    "- that machine learning algorithms can use for training and prediction. \n",
    "- This step is critical because **Spark ML models** require input features to be in **vector form**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d230f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÙŠØ§Ø¨Ø±Ù†Ø³ ğŸ¦¦\n",
      "âœ… ØªÙ… ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
      "\n",
      "ğŸ”¹(Base Models)...\n",
      "train, Logistic Regression...\n",
      "âœ… complete\n",
      "train, Decision Tree...\n",
      "âœ… complete\n",
      "train, Gradient Boosted Trees...\n",
      "âœ… complete\n",
      "\n",
      "ğŸ”¹ Create Level 2 Meta Features\n",
      "add predictions LR...\n",
      "add predictions DT...\n",
      "add predictions GBT...\n",
      "Ø¯Ù…Ø¬ Ø§Ù„predictions...\n",
      "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Meta\n",
      "train, Meta Model (final model)...\n",
      "âœ… complete train, Stacking Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Apply Feature Assembly\n",
    "train_data_assembled = assembler.transform(train_data).cache()\n",
    "test_data_assembled = assembler.transform(test_data).cache()\n",
    "\n",
    "print(\"ÙŠØ§Ø¨Ø±Ù†Ø³ ğŸ¦¦\")\n",
    "print(\"âœ… ØªÙ… ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª\")\n",
    "#=================================================================================\n",
    "\n",
    "# Cell 10: Define Base Models with Stronger Regularization\n",
    "print(\"\\nğŸ”¹(Base Models)...\")\n",
    "\n",
    "# Model 1: Logistic Regression\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"IsHotspot\",\n",
    "    maxIter=50,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.5,\n",
    "    probabilityCol=\"lr_probability\",\n",
    "    predictionCol=\"lr_prediction\",\n",
    "    rawPredictionCol=\"lr_rawPrediction\"\n",
    ")\n",
    "\n",
    "# Model 2: Decision Tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"IsHotspot\",\n",
    "    maxDepth=5,\n",
    "    minInstancesPerNode=10, # Ø¹ÙŠØ§Ù„ÙŠ ÙˆØ§Ø®ÙˆØ§Øª Ø¹ÙŠØ§Ù„ÙŠ\n",
    "    probabilityCol=\"dt_probability\",\n",
    "    predictionCol=\"dt_prediction\",\n",
    "    rawPredictionCol=\"dt_rawPrediction\"\n",
    ")\n",
    "\n",
    "# Model 3: Gradient Boosted Trees\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"IsHotspot\",\n",
    "    maxIter=20,\n",
    "    maxDepth=3,\n",
    "    stepSize=0.1,#learning rate\n",
    "    predictionCol=\"gbt_prediction\"\n",
    ")\n",
    "\n",
    "# Cell 11: Train Base Models\n",
    "print(\"train, Logistic Regression...\")\n",
    "model_lr = lr.fit(train_data_assembled)\n",
    "print(\"âœ… complete\")\n",
    "\n",
    "print(\"train, Decision Tree...\")\n",
    "model_dt = dt.fit(train_data_assembled)\n",
    "print(\"âœ… complete\")\n",
    "\n",
    "print(\"train, Gradient Boosted Trees...\")\n",
    "model_gbt = gbt.fit(train_data_assembled)\n",
    "print(\"âœ… complete\")\n",
    "\n",
    "#================================================================== meta model\n",
    "\n",
    "# Cell 12: Create Level 2 Meta Features\n",
    "print(\"\\nğŸ”¹ Create Level 2 Meta Features\")\n",
    "train_with_id = train_data_assembled.withColumn(\"row_id\", monotonically_increasing_id())#id_name of each record\n",
    "\n",
    "print(\"add predictions LR...\")\n",
    "lr_preds = model_lr.transform(train_with_id).select(\n",
    "    \"row_id\",\n",
    "    vector_to_array(col(\"lr_probability\"))[1].alias(\"lr_prob\") #[1] = probability Ø¥Ù† Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Hotspot.\n",
    ")\n",
    "\n",
    "print(\"add predictions DT...\")\n",
    "dt_preds = model_dt.transform(train_with_id).select(\n",
    "    \"row_id\",\n",
    "    vector_to_array(col(\"dt_probability\"))[1].alias(\"dt_prob\")\n",
    ")\n",
    "\n",
    "print(\"add predictions GBT...\")\n",
    "gbt_preds = model_gbt.transform(train_with_id).select(\n",
    "    \"row_id\",\n",
    "    col(\"gbt_prediction\").alias(\"gbt_pred\")\n",
    ")\n",
    "\n",
    "# Cell 13: Merge Predictions\n",
    "print(\"Ø¯Ù…Ø¬ Ø§Ù„predictions...\")\n",
    "train_meta_features = train_with_id.select(\"row_id\", \"IsHotspot\") \\\n",
    "    .join(lr_preds, \"row_id\") \\\n",
    "    .join(dt_preds, \"row_id\") \\\n",
    "    .join(gbt_preds, \"row_id\") \\\n",
    "    .drop(\"row_id\")\n",
    "\n",
    "print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Meta\")\n",
    "\n",
    "meta_feature_cols = [\"lr_prob\", \"dt_prob\", \"gbt_pred\"]\n",
    "meta_assembler = VectorAssembler(inputCols=meta_feature_cols, outputCol=\"meta_features\")\n",
    "\n",
    "train_meta_data = meta_assembler.transform(train_meta_features).cache()\n",
    "\n",
    "\n",
    "# Cell 14: Train Meta Model\n",
    "print(\"train, Meta Model (final model)...\")\n",
    "lr_meta = LogisticRegression(\n",
    "    featuresCol=\"meta_features\",\n",
    "    labelCol=\"IsHotspot\",\n",
    "    maxIter=100,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "model_meta = lr_meta.fit(train_meta_data)\n",
    "print(\"âœ… complete train, Stacking Ensemble\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9d958c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the model...\n"
     ]
    }
   ],
   "source": [
    "# Prepare Test Data\n",
    "print(\"\\nEvaluating the model...\")\n",
    "test_with_id = test_data_assembled.withColumn(\"row_id\", monotonically_increasing_id())# n\n",
    "\n",
    "test_lr_preds = model_lr.transform(test_with_id).select(\n",
    "    \"row_id\",\n",
    "    vector_to_array(col(\"lr_probability\"))[1].alias(\"lr_prob\"),\n",
    "    col(\"lr_rawPrediction\").alias(\"lr_raw\")\n",
    ")\n",
    "\n",
    "test_dt_preds = model_dt.transform(test_with_id).select(\n",
    "    \"row_id\",\n",
    "    vector_to_array(col(\"dt_probability\"))[1].alias(\"dt_prob\"),\n",
    "    col(\"dt_rawPrediction\").alias(\"dt_raw\")\n",
    ")\n",
    "\n",
    "test_gbt_preds = model_gbt.transform(test_with_id).select(\n",
    "    \"row_id\",\n",
    "    col(\"gbt_prediction\").alias(\"gbt_pred\")\n",
    ")\n",
    "\n",
    "# Make Final Predictions\n",
    "test_meta_features = test_with_id.select(\"row_id\", \"IsHotspot\") \\\n",
    "    .join(test_lr_preds, \"row_id\") \\\n",
    "    .join(test_dt_preds, \"row_id\") \\\n",
    "    .join(test_gbt_preds, \"row_id\") \\\n",
    "    .drop(\"row_id\")\n",
    "\n",
    "test_meta_data = meta_assembler.transform(test_meta_features)\n",
    "predictions = model_meta.transform(test_meta_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a006b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+-------+--------------------+--------+---------+--------+---------+-------------+---------+----+-----+-----------+------------+--------------+------------+-------------+-----------------------+--------------------+\n",
      "|lat_grid|lon_grid|      date|  state|      city_or_county|latitude|longitude|n_killed|n_injured|total_victims|IsHotspot|year|month|day_of_week|day_of_month|incident_count|total_killed|total_injured|avg_killed_per_incident|            features|\n",
      "+--------+--------+----------+-------+--------------------+--------+---------+--------+---------+-------------+---------+----+-----+-----------+------------+--------------+------------+-------------+-----------------------+--------------------+\n",
      "|  25.727|  -80.25|2016-01-26|Florida|Miami (Coconut Gr...|  25.727| -80.2496|       0|        2|            2|        0|2016|    1|          3|          26|             1|           0|            2|                    0.0|[1.0,3.0,26.0,25....|\n",
      "|   25.85| -80.225|2015-07-23|Florida| Miami-Dade (county)| 25.8498| -80.2252|       0|        1|            1|        0|2015|    7|          5|          23|             2|           0|            4|                    0.0|[7.0,5.0,23.0,25....|\n",
      "|  25.855|  -80.18|2014-06-24|Florida|               Miami| 25.8545| -80.1797|       1|        0|            1|        0|2014|    6|          3|          24|             1|           1|            0|                    1.0|[6.0,3.0,24.0,25....|\n",
      "|  25.869| -80.223|2018-01-03|Florida|               Miami| 25.8691| -80.2231|       1|        0|            1|        0|2018|    1|          4|           3|             1|           1|            0|                    1.0|[1.0,4.0,3.0,25.8...|\n",
      "|  26.139| -81.795|2016-06-19|Florida|              Naples| 26.1385| -81.7952|       0|        0|            0|        0|2016|    6|          1|          19|             1|           0|            0|                    0.0|[6.0,1.0,19.0,26....|\n",
      "|  26.196| -80.142|2016-11-01|Florida|Fort Lauderdale (...| 26.1962| -80.1423|       0|        0|            0|        0|2016|   11|          3|           1|             1|           0|            0|                    0.0|[11.0,3.0,1.0,26....|\n",
      "|  26.211| -81.744|2014-03-30|Florida|              Naples| 26.2114|  -81.744|       0|        0|            0|        0|2014|    3|          1|          30|             1|           0|            0|                    0.0|[3.0,1.0,30.0,26....|\n",
      "|  26.259| -80.207|2015-04-01|Florida|             Margate| 26.2589| -80.2073|       2|        0|            2|        1|2015|    4|          4|           1|             7|          14|            0|                    2.0|[4.0,4.0,1.0,26.2...|\n",
      "|  26.259| -80.207|2015-04-01|Florida|             Margate| 26.2589| -80.2073|       2|        0|            2|        1|2015|    4|          4|           1|             7|          14|            0|                    2.0|[4.0,4.0,1.0,26.2...|\n",
      "|  26.544| -82.007|2018-03-18|Florida|          Cape Coral| 26.5444| -82.0068|       0|        0|            0|        0|2018|    3|          1|          18|             1|           0|            0|                    0.0|[3.0,1.0,18.0,26....|\n",
      "+--------+--------+----------+-------+--------------------+--------+---------+--------+---------+-------------+---------+----+-----+-----------+------------+--------------+------------+-------------+-----------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (test_data_assembled.show(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76fbec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Model AUC-ROC: 0.9794\n",
      "ğŸ¯ Classification Accuracy: 0.9623\n",
      "ğŸ¯ F1-Score: 0.9629\n",
      "ğŸ¯ Precision: 0.9647\n",
      "ğŸ¯ Recall: 0.9623\n",
      "\n",
      "Sample predictions:\n",
      "+---------+----------+--------------------+\n",
      "|IsHotspot|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|        0|       1.0|[0.24602423996822...|\n",
      "|        0|       0.0|[0.93534119884147...|\n",
      "|        0|       0.0|[0.93534119884147...|\n",
      "|        1|       1.0|[0.24602423996822...|\n",
      "|        0|       0.0|[0.93534119884147...|\n",
      "|        1|       1.0|[0.24602423996822...|\n",
      "|        0|       1.0|[0.24602423996822...|\n",
      "|        0|       0.0|[0.93534119884147...|\n",
      "|        1|       1.0|[0.24602423996822...|\n",
      "|        1|       1.0|[0.24602423996822...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "ğŸ“Š Confusion Matrix:\n",
      "==================================================\n",
      "+---------+----------+-----+\n",
      "|IsHotspot|prediction|count|\n",
      "+---------+----------+-----+\n",
      "|        0|       0.0|32491|\n",
      "|        0|       1.0| 1369|\n",
      "|        1|       0.0|  328|\n",
      "|        1|       1.0|10840|\n",
      "+---------+----------+-----+\n",
      "\n",
      "\n",
      "Class 0 Performance:\n",
      "  Total samples: 33860\n",
      "  Correct predictions: 32491\n",
      "  Class Accuracy: 0.9596\n",
      "\n",
      "Class 1 Performance:\n",
      "  Total samples: 11168\n",
      "  Correct predictions: 10840\n",
      "  Class Accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Evaluation\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"IsHotspot\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"\\nğŸ“Š Model AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"IsHotspot\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"ğŸ¯ Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"IsHotspot\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "print(f\"ğŸ¯ F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"IsHotspot\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "print(f\"ğŸ¯ Precision: {precision:.4f}\")\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"IsHotspot\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "print(f\"ğŸ¯ Recall: {recall:.4f}\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "predictions.select(\"IsHotspot\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Cell 18: Confusion Matrix\n",
    "print(\"\\nğŸ“Š Confusion Matrix:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "confusion_matrix = predictions.groupBy(\"IsHotspot\", \"prediction\").count()\n",
    "confusion_matrix.orderBy(\"IsHotspot\", \"prediction\").show()\n",
    "\n",
    "for class_label in [0, 1]:\n",
    "    class_predictions = predictions.filter(col(\"IsHotspot\") == class_label)\n",
    "    total = class_predictions.count()\n",
    "    correct = class_predictions.filter(col(\"prediction\") == class_label).count()\n",
    "    class_accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nClass {class_label} Performance:\")\n",
    "    print(f\"  Total samples: {total}\")\n",
    "    print(f\"  Correct predictions: {correct}\")\n",
    "    print(f\"  Class Accuracy: {class_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e1158bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+--------------------+----------------+--------+--------------------+--------------------+--------------------+----------+\n",
      "|IsHotspot|            lr_prob|              lr_raw|             dt_prob|          dt_raw|gbt_pred|       meta_features|       rawPrediction|         probability|prediction|\n",
      "+---------+-------------------+--------------------+--------------------+----------------+--------+--------------------+--------------------+--------------------+----------+\n",
      "|        0| 0.2863982059115956|[0.91294192460826...|  0.9095530368641036|[2402.0,24155.0]|     1.0|[0.28639820591159...|[-1.1199301514580...|[0.24602423996822...|       1.0|\n",
      "|        0|0.22862181130659587|[1.21610961106219...|0.002718351591594857| [73374.0,200.0]|     0.0|[0.22862181130659...|[2.67178714984083...|[0.93534119884147...|       0.0|\n",
      "|        0| 0.2322056661255283|[1.19589843196526...|0.002718351591594857| [73374.0,200.0]|     0.0|[0.23220566612552...|[2.67178714984083...|[0.93534119884147...|       0.0|\n",
      "|        1| 0.2701647942951424|[0.99378664099597...|  0.9095530368641036|[2402.0,24155.0]|     1.0|[0.27016479429514...|[-1.1199301514580...|[0.24602423996822...|       1.0|\n",
      "|        0|0.22862181130659587|[1.21610961106219...|0.002718351591594857| [73374.0,200.0]|     0.0|[0.22862181130659...|[2.67178714984083...|[0.93534119884147...|       0.0|\n",
      "|        1| 0.2905466190396043|[0.89273074551133...|  0.9095530368641036|[2402.0,24155.0]|     1.0|[0.29054661903960...|[-1.1199301514580...|[0.24602423996822...|       1.0|\n",
      "|        0|0.26226891597291935|[1.03420899918983...|  0.9095530368641036|[2402.0,24155.0]|     1.0|[0.26226891597291...|[-1.1199301514580...|[0.24602423996822...|       1.0|\n",
      "|        0| 0.2469299207339185|[1.11505371557754...|0.002718351591594857| [73374.0,200.0]|     0.0|[0.24692992073391...|[2.67178714984083...|[0.93534119884147...|       0.0|\n",
      "|        1|0.28228545620011913|[0.93315310370519...|  0.9095530368641036|[2402.0,24155.0]|     1.0|[0.28228545620011...|[-1.1199301514580...|[0.24602423996822...|       1.0|\n",
      "|        1| 0.2989488449119545|[0.85230838731747...|  0.9095530368641036|[2402.0,24155.0]|     1.0|[0.29894884491195...|[-1.1199301514580...|[0.24602423996822...|       1.0|\n",
      "+---------+-------------------+--------------------+--------------------+----------------+--------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(predictions.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7a255159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|         probability|prediction|\n",
      "+--------------------+----------+\n",
      "|[0.24602423996822...|       1.0|\n",
      "|[0.93534119884147...|       0.0|\n",
      "|[0.93534119884147...|       0.0|\n",
      "|[0.24602423996822...|       1.0|\n",
      "|[0.93534119884147...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "final_pr=predictions.select(col(\"probability\"), col(\"prediction\"))\n",
    "print(final_pr.show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2c5c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Alternative approach: use selectExpr to extract probability value from vector and compare\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import DoubleType\\n\\n# Assume \\'probability\\' is a vector, so extract probability of class 1 (e.g. probability[1])\\nextract_prob = udf(lambda v: float(v[1]), DoubleType())\\nfinal_pr_with_prob = final_pr.withColumn(\"prob_1\", extract_prob(col(\"probability\")))\\n\\nhigh_risk = final_pr_with_prob.filter(\\n    (col(\"prediction\") == 1) & (col(\"prob_1\") > 0.7)\\n)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Alternative approach: use selectExpr to extract probability value from vector and compare\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Assume 'probability' is a vector, so extract probability of class 1 (e.g. probability[1])\n",
    "extract_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "final_pr_with_prob = final_pr.withColumn(\"prob_1\", extract_prob(col(\"probability\")))\n",
    "\n",
    "high_risk = final_pr_with_prob.filter(\n",
    "    (col(\"prediction\") == 1) & (col(\"prob_1\") > 0.7)\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f8cbd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Model Comparison:\n",
      "==================================================\n",
      "Logistic Regression - AUC: 0.9557, Accuracy: 0.6424, F1: 0.6424\n",
      "Decision Tree - AUC: 0.9590, Accuracy: 0.9637, F1: 0.9637\n",
      "Gradient Boosted Trees - AUC: 0.9876, Accuracy: 0.9629, F1: 0.9629\n",
      "\n",
      "ğŸ† Stacking Ensemble - AUC: 0.9794, Accuracy: 0.9623, F1: 0.9629\n",
      "==================================================\n",
      "\n",
      "âœ… Complete model evaluation\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Compare Models\n",
    "print(\"\\nğŸ“ˆ Model Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_predictions_lr = test_with_id.join(test_lr_preds, \"row_id\").withColumnRenamed(\"lr_raw\", \"rawPrediction\")\n",
    "test_predictions_dt = test_with_id.join(test_dt_preds, \"row_id\").withColumnRenamed(\"dt_raw\", \"rawPrediction\")\n",
    "\n",
    "auc_lr = evaluator.evaluate(test_predictions_lr)\n",
    "acc_lr = MulticlassClassificationEvaluator(labelCol=\"IsHotspot\", predictionCol=\"lr_prediction\").evaluate(\n",
    "    model_lr.transform(test_with_id)\n",
    ")\n",
    "f1_lr = MulticlassClassificationEvaluator(labelCol=\"IsHotspot\", predictionCol=\"lr_prediction\", metricName=\"f1\").evaluate(\n",
    "    model_lr.transform(test_with_id)\n",
    ")\n",
    "print(f\"Logistic Regression - AUC: {auc_lr:.4f}, Accuracy: {acc_lr:.4f}, F1: {f1_lr:.4f}\")\n",
    "\n",
    "auc_dt = evaluator.evaluate(test_predictions_dt)\n",
    "acc_dt = MulticlassClassificationEvaluator(labelCol=\"IsHotspot\", predictionCol=\"dt_prediction\").evaluate(\n",
    "    model_dt.transform(test_with_id)\n",
    ")\n",
    "f1_dt = MulticlassClassificationEvaluator(labelCol=\"IsHotspot\", predictionCol=\"dt_prediction\", metricName=\"f1\").evaluate(\n",
    "    model_dt.transform(test_with_id)\n",
    ")\n",
    "print(f\"Decision Tree - AUC: {auc_dt:.4f}, Accuracy: {acc_dt:.4f}, F1: {f1_dt:.4f}\")\n",
    "\n",
    "test_gbt_full = model_gbt.transform(test_with_id)\n",
    "auc_gbt = evaluator.evaluate(test_gbt_full)\n",
    "acc_gbt = MulticlassClassificationEvaluator(labelCol=\"IsHotspot\", predictionCol=\"gbt_prediction\").evaluate(test_gbt_full)\n",
    "f1_gbt = MulticlassClassificationEvaluator(labelCol=\"IsHotspot\", predictionCol=\"gbt_prediction\", metricName=\"f1\").evaluate(test_gbt_full)\n",
    "print(f\"Gradient Boosted Trees - AUC: {auc_gbt:.4f}, Accuracy: {acc_gbt:.4f}, F1: {f1_gbt:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ† Stacking Ensemble - AUC: {auc:.4f}, Accuracy: {accuracy:.4f}, F1: {f1_score:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cell 20: Memory Cleanup\n",
    "train_data_assembled.unpersist()\n",
    "test_data_assembled.unpersist()\n",
    "train_meta_data.unpersist()\n",
    "\n",
    "print(\"\\nâœ… Complete model evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d599afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.toPandas().to_csv(\"df_final_export.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5121996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: 301677 ØµÙ\n",
      "root\n",
      " |-- lat_grid: double (nullable = true)\n",
      " |-- lon_grid: double (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city_or_county: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- n_killed: integer (nullable = true)\n",
      " |-- n_injured: integer (nullable = true)\n",
      " |-- total_victims: integer (nullable = true)\n",
      " |-- IsHotspot: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- incident_count: integer (nullable = true)\n",
      " |-- total_killed: integer (nullable = true)\n",
      " |-- total_injured: integer (nullable = true)\n",
      " |-- avg_killed_per_incident: double (nullable = true)\n",
      "\n",
      "+--------+--------+----------+-------+--------------+--------+---------+--------+---------+-------------+---------+----+-----+-----------+------------+--------------+------------+-------------+-----------------------+\n",
      "|lat_grid|lon_grid|      date|  state|city_or_county|latitude|longitude|n_killed|n_injured|total_victims|IsHotspot|year|month|day_of_week|day_of_month|incident_count|total_killed|total_injured|avg_killed_per_incident|\n",
      "+--------+--------+----------+-------+--------------+--------+---------+--------+---------+-------------+---------+----+-----+-----------+------------+--------------+------------+-------------+-----------------------+\n",
      "|  19.448|-155.189|2015-09-04| Hawaii|       Volcano| 19.4475| -155.189|       0|        0|            0|        0|2015|    9|          6|           4|             1|           0|            0|                    0.0|\n",
      "|   25.54| -80.513|2016-12-20|Florida|     Homestead| 25.5399| -80.5126|       0|        1|            1|        0|2016|   12|          3|          20|             1|           0|            1|                    0.0|\n",
      "|  25.551| -80.405|2014-01-03|Florida|         Miami| 25.5514| -80.4047|       0|        0|            0|        0|2014|    1|          6|           3|             1|           0|            0|                    0.0|\n",
      "|  25.608| -80.373|2014-09-02|Florida|    Miami-dade| 25.6077| -80.3734|       0|        1|            1|        0|2014|    9|          3|           2|             1|           0|            1|                    0.0|\n",
      "|  25.686|  -80.38|2014-08-31|Florida|       Kendall| 25.6864| -80.3804|       0|        1|            1|        0|2014|    8|          1|          31|             1|           0|            1|                    0.0|\n",
      "+--------+--------+----------+-------+--------------+--------+---------+--------+---------+-------------+---------+----+-----+-----------+------------+--------------+------------+-------------+-----------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "file_path = \"df_final_export.csv\"  \n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df.count()} ØµÙ\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
